import os
import torch
import torchaudio
import pandas as pd
import numpy as np
from torch.utils.data import Dataset

class EmotionSegmentDataset(Dataset):
    def __init__(self, csv_path, project_root=".", sample_rate=16000, max_duration=6.0):
        self.df = pd.read_csv(csv_path)
        self.project_root = project_root
        self.sample_rate = sample_rate
        self.max_length = int(max_duration * sample_rate)

        # è‡ªåŠ¨è¯†åˆ«éŸ³é¢‘è·¯å¾„åˆ—å
        possible_keys = ['wav_path', 'wav', 'path', 'filename', 'file_path']
        self.path_col = None
        for key in possible_keys:
            if key in self.df.columns:
                self.path_col = key
                break
        
        if self.path_col is None:
            raise KeyError(f"âŒ CSV ä¸­æ‰¾ä¸åˆ°éŸ³é¢‘è·¯å¾„åˆ—! è¯·ç¡®ä¿åˆ—åæ˜¯ä»¥ä¸‹ä¹‹ä¸€: {possible_keys}")
        
        self.resample_transforms = {} 

    def __len__(self):
        return len(self.df)

    def _get_resampler(self, orig_freq):
        if orig_freq not in self.resample_transforms:
            self.resample_transforms[orig_freq] = torchaudio.transforms.Resample(
                orig_freq=orig_freq, new_freq=self.sample_rate
            )
        return self.resample_transforms[orig_freq]

    def __getitem__(self, idx):
        # ===============================================================
        # ğŸ”´ å…¨å±€ Try-Except ä¿æŠ¤ï¼šé˜²æ­¢ä»»ä½•ä¸€æ¡åæ•°æ®æå´©æ•´ä¸ªè®­ç»ƒ
        # ===============================================================
        try:
            row = self.df.iloc[idx]
            
            # 1. è·¯å¾„å¤„ç†
            wav_relative_path = row[self.path_col]
            full_wav_path = os.path.join(self.project_root, wav_relative_path)

            # 2. åŠ è½½éŸ³é¢‘
            if not os.path.exists(full_wav_path):
                raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {full_wav_path}")

            waveform, sr = torchaudio.load(full_wav_path)

            # 3. é‡é‡‡æ ·
            if sr != self.sample_rate:
                resampler = self._get_resampler(sr)
                waveform = resampler(waveform)

            # 4. è½¬å•å£°é“
            if waveform.shape[0] > 1:
                waveform = torch.mean(waveform, dim=0, keepdim=True)

            # 5. åˆ‡ç‰‡
            start_sec = float(row['start'])
            end_sec = float(row['end'])
            
            # æ£€æŸ¥ NaN
            if pd.isna(start_sec) or pd.isna(end_sec):
                raise ValueError("æ—¶é—´æˆ³åŒ…å« NaN")

            start_frame = int(start_sec * self.sample_rate)
            end_frame = int(end_sec * self.sample_rate)

            if end_frame > waveform.shape[1]:
                end_frame = waveform.shape[1]
            
            if start_frame >= end_frame:
                # å¦‚æœåˆ‡ç‰‡æ— æ•ˆï¼Œè¿”å›é™éŸ³
                cropped_wave = torch.zeros(1, 16000)
            else:
                cropped_wave = waveform[:, start_frame:end_frame]

            # 6. ç»Ÿä¸€é•¿åº¦ (Pad/Truncate)
            current_len = cropped_wave.shape[1]
            if current_len > self.max_length:
                cropped_wave = cropped_wave[:, :self.max_length]
            elif current_len < self.max_length:
                pad_amount = self.max_length - current_len
                cropped_wave = torch.nn.functional.pad(cropped_wave, (0, pad_amount))

            # 7. æ ‡ç­¾å¤„ç†
            act = float(row['activation'])
            val = float(row['valence'])
            dom = float(row['dominance'])

            # æ£€æŸ¥æ ‡ç­¾æ˜¯å¦ä¸º NaN
            if pd.isna(act) or pd.isna(val) or pd.isna(dom):
                raise ValueError(f"æ ‡ç­¾åŒ…å« NaN: {act}, {val}, {dom}")

            # å½’ä¸€åŒ– (1-5 -> 0-1)
            raw_labels = [act, val, dom]
            norm_labels = [(x - 1.0) / 4.0 for x in raw_labels]
            labels = torch.tensor(norm_labels, dtype=torch.float32)

            return cropped_wave, labels

        except Exception as e:
            # ===============================================================
            # ğŸ”´ é”™è¯¯æ•è·åŒº
            # ===============================================================
            # æ‰“å°å‡ºé”™çš„æ–‡ä»¶ï¼Œæ–¹ä¾¿ä½ æ’æŸ¥
            print(f"\nâš ï¸ æ•°æ®åŠ è½½è­¦å‘Š [Index {idx}]: {e}")
            # print(f"å‡ºé”™æ–‡ä»¶: {row[self.path_col] if 'row' in locals() else 'Unknown'}")
            
            # è¿”å›â€œå‡æ•°æ®â€ (Dummy Data) ä¿è¯ç¨‹åºä¸å´©æºƒ
            # è¿”å› 1ç§’çš„é™éŸ³ + æ ‡ç­¾[0.5, 0.5, 0.5] (ä»£è¡¨ä¸­æ€§æƒ…ç»ª)
            dummy_wav = torch.zeros(1, self.max_length)
            dummy_label = torch.tensor([0.5, 0.5, 0.5], dtype=torch.float32)
            return dummy_wav, dummy_label