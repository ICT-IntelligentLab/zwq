import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from tqdm import tqdm

from src.dataset import EmotionSegmentDataset
from src.model import DynamicEmotionModel

# ================= é…ç½® =================
BATCH_SIZE = 8          # å†»ç»“æ¨¡å¼ä¸‹å¯ä»¥å¤§ä¸€ç‚¹
LR = 1e-4               # æ ‡å‡†å­¦ä¹ ç‡
EPOCHS = 30             
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
WAVLM_PATH = "pretrained/wavlm.pt"
SAVE_DIR = "models_fusion" # æ”¹ä¸ªåå­—ï¼Œè·Ÿä¹‹å‰çš„åŒºåˆ†å¼€
# =======================================

class CCCLoss(nn.Module):
    def __init__(self):
        super().__init__()
    def forward(self, x, y):
        x_mean, y_mean = torch.mean(x, dim=0), torch.mean(y, dim=0)
        x_var, y_var = torch.var(x, dim=0, unbiased=False), torch.var(y, dim=0, unbiased=False)
        cov = torch.mean((x - x_mean) * (y - y_mean), dim=0)
        ccc = (2 * cov) / (x_var + y_var + (x_mean - y_mean) ** 2 + 1e-8)
        return 1.0 - torch.mean(ccc)

def train():
    os.makedirs(SAVE_DIR, exist_ok=True)
    print(f"ğŸš€ å¯åŠ¨å¤šå±‚èåˆè®­ç»ƒ (Layer-wise Fusion)...")

    # 1. æ•°æ®
    train_ds = EmotionSegmentDataset("data/train.csv", project_root=".")
    val_ds = EmotionSegmentDataset("data/val.csv", project_root=".")
    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)
    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)

    # 2. æ¨¡å‹
    model = DynamicEmotionModel(wavlm_path=WAVLM_PATH).to(DEVICE)
    
    # ğŸ”´ å…³é”®ï¼šå†»ç»“ WavLM å†…éƒ¨å‚æ•°ï¼Œä½†å…è®¸è®­ç»ƒ layer_weights
    # model.parameters() ä¼šåŒ…å« layer_weightsï¼Œæ‰€ä»¥ä¼˜åŒ–å™¨ä¼šè‡ªåŠ¨å¤„ç†
    for name, param in model.named_parameters():
        if "wavlm" in name:
            param.requires_grad = False
        else:
            # åŒ…æ‹¬ transformer, regressor å’Œ layer_weights
            param.requires_grad = True
    
    print("âœ… WavLM ä¸»ä½“å·²å†»ç»“ï¼Œä»…è®­ç»ƒå±‚æƒé‡é€‚é…å™¨å’Œä¸‹æ¸¸ç½‘ç»œã€‚")

    # 3. ä¼˜åŒ–å™¨
    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LR, weight_decay=1e-2)
    mse_criterion = nn.MSELoss()
    ccc_criterion = CCCLoss()

    best_loss = float('inf')

    # 4. å¾ªç¯
    for epoch in range(EPOCHS):
        model.train()
        train_loss = 0.0
        loop = tqdm(train_loader, desc=f"Epoch {epoch+1}/{EPOCHS} [Train]")
        
        for wav, labels in loop:
            wav, labels = wav.to(DEVICE), labels.to(DEVICE)
            
            preds = model(wav)
            loss = 0.5 * mse_criterion(preds, labels) + 0.5 * ccc_criterion(preds, labels)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            loop.set_postfix(loss=loss.item())

        # éªŒè¯
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for wav, labels in tqdm(val_loader, desc=f"Epoch {epoch+1}/{EPOCHS} [Val]"):
                wav, labels = wav.to(DEVICE), labels.to(DEVICE)
                preds = model(wav)
                l_mse = mse_criterion(preds, labels)
                l_ccc = ccc_criterion(preds, labels)
                val_loss += (0.5 * l_mse + 0.5 * l_ccc).item()

        avg_val = val_loss / len(val_loader)
        avg_train = train_loss / len(train_loader)
        
        print(f"Epoch {epoch+1}: Train={avg_train:.4f} | Val={avg_val:.4f}")

        if avg_val < best_loss:
            best_loss = avg_val
            torch.save(model.state_dict(), os.path.join(SAVE_DIR, "best_model.pth"))
            print(f"ğŸ‰ ä¿å­˜æœ€ä½³æ¨¡å‹ (Loss: {best_loss:.4f})")
            
            # æ‰“å°ä¸€ä¸‹å½“å‰çš„å±‚æƒé‡ï¼Œçœ‹çœ‹æ¨¡å‹æ›´å–œæ¬¢å“ªä¸€å±‚
            weights = torch.nn.functional.softmax(model.layer_weights, dim=0).detach().cpu().numpy()
            print(f"ğŸ” å½“å‰å±‚æƒé‡åˆ†å¸ƒ: {[f'{w:.2f}' for w in weights]}")

if __name__ == "__main__":
    train()